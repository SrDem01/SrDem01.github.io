[{"content":"Resources {{video https://www.youtube.com/watch?v=wBVSbVktLIY}} What is a kernel? The linear algebra one.\nSo, the idea is that you can compute some dot product of the feature space with a function of the original vectors.\nThis function is called the kernel function.\n$$k(x_i, x_j) = \\phi(x_i)\u0026rsquo;\\phi(x_j)$$ In linear regression Take a function of the data $\\phi(X)$ as our feature vector.\nFor estimation, we usually use\n$$\\hat{\\beta} = (X\u0026rsquo;X)^{-1}XY$$ If we had $\\phi(X)$ as our feature vector instead,\n$$\\hat{\\beta} = (\\phi(X)\u0026rsquo;\\phi(X))^{-1}\\phi(X)Y$$ Apparently, it is really difficult to compute the $ bit.\nThis is also known as the covariance matrix. With some simplification, we can rewrite the solution as\n$$\\phi(X)\u0026rsquo; (\\phi(X) \\phi(X)\u0026rsquo;)^{-1}Y$$ The insides of the $\\phi(X)\\phi(X)\u0026rsquo;$ are all dot products.\nBut a kernel function compute this dot product using the $x_i$ directly.\nWhich is much faster, I suppose.\nThe matrix just needs to be symmetric and positive semi-definite.\nMercer\u0026rsquo;s theorem Which it is for the polynomial regression.\nRewriting with the kernel matrix, $K$\n$$\\hat{\\beta}=\\phi(X)\u0026rsquo; K^{-1}Y$$ We still need $\\phi$ for weights\nBut if we want to make predictions we need $\\hat{y} = \\hat{\\beta}\u0026rsquo;\\phi(x)$.\nPlugging in the value of the $\\hat{\\beta}$,\n$$\\hat{y} = Y K^{-1}\\phi(X)\u0026rsquo; \\phi(x)$$ The term $\\phi(X)\u0026rsquo;\\phi(x)$ is another vector of dot products\nThis can be replaced with a vector of kernel functions.\n$$\\hat{y} = Y K^{-1}k_x$$ The fundamental point is that you do not need $\\phi$ to predict at all.\nSo, you can just start with a kernel only directly\nIt will implicitly define a feature space on its own.\nCommon Kernels Linear\nPolynomial\nGaussian\n","permalink":"https://example.com/pages/kernel-regression/","summary":"Resources {{video https://www.youtube.com/watch?v=wBVSbVktLIY}} What is a kernel? The linear algebra one.\nSo, the idea is that you can compute some dot product of the feature space with a function of the original vectors.\nThis function is called the kernel function.\n$$k(x_i, x_j) = \\phi(x_i)\u0026rsquo;\\phi(x_j)$$ In linear regression Take a function of the data $\\phi(X)$ as our feature vector.\nFor estimation, we usually use\n$$\\hat{\\beta} = (X\u0026rsquo;X)^{-1}XY$$ If we had $\\phi(X)$ as our feature vector instead,","title":"Kernel Regression"},{"content":"Resources Single-index model - Wikipedia Model It regresses the excess (over the risk free rate) return of a stock on the excess return of the market in the same period.\nThis market return is captured by a single index. Something like nifty 100 or s\u0026amp;p 500.\nThe regression eq:\n$$r_{it} - r_f = \\alpha + \\beta_i (r_{mt} - r_f) + \\epsilon_{it}$$\nerrors $$\\sim N(0, \\sigma^2)$$\nmarket term represents the movement of the market caused by macroeconomic changes.\n\\beta are the responsiveness of the firm to these macroeconomic variables.\n\\epsilon represents firm-specific shocks.\nCovariance between stocks Assumes that returns between stocks are uncorrelated post subtracting the market means.\n$$Cov(R_{it}- \\beta_i m_t, R_{jt} - \\beta_j m_t) = 0 \\ $$ You can get the covariance by multiplying the \\beta s and the market variance.\n$$Cov(R_i, R_j) = \\beta_j \\beta_i \\sigma^2$$ Basically, the point is to simplify Covariance calculations.\n","permalink":"https://example.com/pages/portfolio-single-index-model/","summary":"Resources Single-index model - Wikipedia Model It regresses the excess (over the risk free rate) return of a stock on the excess return of the market in the same period.\nThis market return is captured by a single index. Something like nifty 100 or s\u0026amp;p 500.\nThe regression eq:\n$$r_{it} - r_f = \\alpha + \\beta_i (r_{mt} - r_f) + \\epsilon_{it}$$\nerrors $$\\sim N(0, \\sigma^2)$$\nmarket term represents the movement of the market caused by macroeconomic changes.","title":"Portfolio Single Index Model"},{"content":"Resources Quantile regression - Wikipedia\nhttps://sites.google.com/site/econometricsacademy/econometrics-models/quantile-regression\nPretty good. Watch the videos. + Has a lot of extra stuff that i have not looked at yet. + goes into the maths. The model A linear model:\n$$y_i = x_i \\beta_q + \\epsilon_i$$\na \\beta for every quantile.\nTo compute OLS, we use the following loss function:\n$$\\min\\sum e_i^2$$ Use a new loss function that penalizes differently for under estimation and over estimation.\nIt is apparently linear/proportional(? idk the term)\nTo get 5%ile\npenalize underestimation and overestimation in the ratio 5:95.\nFor quantile $$q$$,\n$$\\min q\\sum_{y_i \\ge \\hat{y_i}} |e_i| + (1-q)\\sum_{y_i \u0026lt; \\hat{y_i}}|e_i|$$ It is easy to see that for the median,\n$$\\min \\sum |e_i|$$ Interpretation Pretty much the same as OLS.\nThe \\beta s turn out to be the marginal effects\n$$\\frac{\\partial Q(y | x)}{\\partial x_i} = \\beta_{qi}$$ at that quantile.\nIt is sorta nice with some what non linear patterns\nWikipedia claims:\nEven when the actual function is non linear, given that the quantile regression still chooses the best linear approximation at that point,\nthe slope parameters $$\\beta_q$$ of the linear model can be interpreted as weighted averages of the [partial] derivatives.\nDunno, where weighted average comes from though.\nBasically, green light for causal inference B-).\nHypothesis testing The \\betas are asymptotically normal, with some scary looking standard errors.\nUse bootstrapping to calculate confidence intervals. Computation [[Linear Programming]].\nThe object function is the loss function.\n$$\\min q\\sum_{y_i \\ge \\hat{y_i}} |e_i| + (1-q)\\sum_{y_i \u0026lt; \\hat{y_i}}|e_i|$$\nwhere $$e_i = y_i - x_i\\beta_q$$\nIt is indeed linear.\nLinear in parameters, remember?\n(we can actually use LP to do OLS also, btw. ) Fancy (correct) Specification Objective fn\n$$\\underset{\\beta,u^{+},u^{-}}{\\min} \\quad{q1^{\u0026rsquo;}u^{+}+(1-q)1^{\u0026rsquo;}u^{-}}$$ such that,\n$$X\\beta+u^{+}-u^{-}=Y\\ \\text{non-negativity constraints on u s}.$$\nNot sure how the betas are made to be over the entire real line\nmaan, you go do LP again. Note that,\n$u^+$ is the underestimation\n$u^-$ is the overestimation.\nFor q = 0.5 The median makes calculations sorta easy\nObjective function\n$$\\min_\\beta 1\u0026rsquo;|Y - X\\beta|$$ We can rewrite that as,\njust a linear algebra trick for converting mods.\n$$\\min_\\beta 1\u0026rsquo;t$$\nwhere,\n$$-t \\le Y - X\\beta \\le t $$ R implementation library(quantreg) # OLS regression olsreg \u0026lt;- lm(Y ~ X, data=mydata) # Quantile regression quantreg25 \u0026lt;- rq(Y ~ X, data=mydata, tau=0.25) # Plotting data quantreg.all \u0026lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05), data=mydata) quantreg.plot \u0026lt;- summary(quantreg.all) plot(quantreg.plot) Math fax In general,\n$$\\bar{x} = \\argmin_y \\sum (x - y)^2 $$\n$$\\text{median} = \\argmin_y \\sum |x - y|$$\nThis gives you the MLE for a laplace distribution in the same way the least squares gives the MLE for a normal distribution.\nAssumptions I really don\u0026rsquo;t them, but i guess they are the gauss-markov ones only?\nand for significance testing, $$\\epsilon_i \\sim N(0, \\sigma^2)$$\nEquivariance Invariant to monotonic transformations,\n$$Q_{h(Y)|X}(q|X) = h(Q_{Y|X}(q|X))$$ Example,\ntake $$Q_{Y|X}(q)=X\\beta_{q}$$\nthen,\n$$Q_{exp(Y)|X}(q)=\\exp(X\\beta_{q})$$\nThe mean regression does not work,\n$$\\operatorname{E} (\\ln(Y))\\neq \\ln(\\operatorname{E}(Y)).$$\nMean regression only works for linear transforms, while quantile ones can handle any monotonic transformation. Pretty cool.\nHeteroskedasticity Panel data methods Because quantiles are not linear operators like expectations,\n$$Q_{a+b}(q) \\ne Q_a(q) + Q_b(q)$$ Demeaning (or differencing) is not possible.\nSo, fixed effects require something fancy.\nread the above pdf. Misc Expectiles.\nThe quantiles but with squared loss instead of deviation.\nwhat does it even mean?\nnothing. bootstrapped quantile regression (BQR) analysis.\nbootstrap amplifies the outliers. Quantile regression averaging\n[[Composite Quantile Regression]]\n","permalink":"https://example.com/pages/quantile-regression/","summary":"Resources Quantile regression - Wikipedia\nhttps://sites.google.com/site/econometricsacademy/econometrics-models/quantile-regression\nPretty good. Watch the videos. + Has a lot of extra stuff that i have not looked at yet. + goes into the maths. The model A linear model:\n$$y_i = x_i \\beta_q + \\epsilon_i$$\na \\beta for every quantile.\nTo compute OLS, we use the following loss function:\n$$\\min\\sum e_i^2$$ Use a new loss function that penalizes differently for under estimation and over estimation.","title":"Quantile regression"},{"content":"Resources Semiparametric regression - Wikipedia + The important one + Quantile regression. Index Models A form of [[Semi Parametric Regression]]\nA link function on the linear combination.\nif the function is known, then non-linear least squares.\nBasically, [[Generalized Linear Models]].\nWe have to estimate the link function and the \\beta\nThe main benefit is that there is no issue with high dimensionality\nThe only non parametric part is the unknown link function.\nPretty good ngl.\nModel The regression model is\n$$E(Y|X) = g(X\\beta)$$\nwhere $$g$$ is an unknown link function.\nThe name Single index comes from the single scalar value that comes out of the $$X\\beta$$.\nIn this sense, all the normal regressions are also SIM.\nwith known link functions. $g$ includes both location and level shift.\nthe $$X$$ does not have an intercept. The \\beta s also need to be normalized to identify the model from an infinite series of equivalent models.\n$\\beta\u0026rsquo;\\beta = 1$ and the first beta is positive.\nOr, set the first \\beta = 1.\n$X$ needs to have more than 2 dims.\notherwise, the \\beta s are pretty much degenerate. Interpretation The marginal effect of some $x_i$ on the conditional mean of $Y$ is\nChain rule.\n$$\\frac{\\partial }{\\partial x_i} g(X\\beta) = \\beta_i g^{-1}(X\\beta)$$\nMethods Ichimura\u0026rsquo;s method what is an NW estimator?\n[[Newey-West estimator]]\nKernel Regression\nQuantile regression [[Composite Quantile Regression]]\nMACE Basic idea It uses a generalized weighted loss function\nThis includes quantile regression. It takes a local linear approximation at $x$ near some $X_i$ of the link function.\nTaylor series?\n$$g(X_i\u0026rsquo;\\beta) \\approx g(X_i\u0026rsquo;\\beta) + g\u0026rsquo;(x\u0026rsquo;\\beta)(X_i - x)\u0026rsquo;\\beta$$\nand minimizes its expected value of the loss function using this linear approximation of the prediction.\nBy minimizing the empirical average.\nwhich for some reason is multiplied with a kernel function?\nhow did the kernel function come? Now this can be solved in two minimization problems\nOne minimizes the loss wrt \\beta given the two functions logseq.order-list-type:: number\nAgain wrt the two functions given \\beta logseq.order-list-type:: number\nYou can also add a penalty parameter to the loss function\ngetting similar results to LASSO This is repeated iteratively\nestimate Î² in (2.10) with an iterative procedure ls-type:: annotation hl-page:: 7 hl-color:: purple to compute\n$\\hat{\\beta}$\n$\\hat{g}(x)$\n$\\hat{g}\u0026rsquo;(x)$\nMisc. Portfolio Single Index Model ","permalink":"https://example.com/pages/single-index-models/","summary":"Resources Semiparametric regression - Wikipedia + The important one + Quantile regression. Index Models A form of [[Semi Parametric Regression]]\nA link function on the linear combination.\nif the function is known, then non-linear least squares.\nBasically, [[Generalized Linear Models]].\nWe have to estimate the link function and the \\beta\nThe main benefit is that there is no issue with high dimensionality\nThe only non parametric part is the unknown link function.","title":"Single Index Models"},{"content":"Concepts Quantile regression\nto estimate VaR Single Index Models\nto estimate CoVaR [[Networks]]\n[[Network Clustering]]\nAnalyse the networks.\nMethodology Variables $X_{it}$ log returns of i\u0026rsquo;th company at t\u0026rsquo;th time.\n$M_{it}$ Macroeconomic variables\n$B_j$ Firm level characteristics.\nnot time subscripted?\nwhyyy?\n$VaR_{it}$ VaR of i\u0026rsquo;th company at t\u0026rsquo;th time.\nMeasures \u0026ldquo;risk\u0026rdquo; of the company.\nHigher values means higher risk.\nUsually estimated at 0.05 level.\nShould we go higher?\nFor more stability?\n","permalink":"https://example.com/pages/tenet-model/","summary":"Concepts Quantile regression\nto estimate VaR Single Index Models\nto estimate CoVaR [[Networks]]\n[[Network Clustering]]\nAnalyse the networks.\nMethodology Variables $X_{it}$ log returns of i\u0026rsquo;th company at t\u0026rsquo;th time.\n$M_{it}$ Macroeconomic variables\n$B_j$ Firm level characteristics.\nnot time subscripted?\nwhyyy?\n$VaR_{it}$ VaR of i\u0026rsquo;th company at t\u0026rsquo;th time.\nMeasures \u0026ldquo;risk\u0026rdquo; of the company.\nHigher values means higher risk.\nUsually estimated at 0.05 level.\nShould we go higher?\nFor more stability?","title":"TENET Model"},{"content":"Papers TENET Model ","permalink":"https://example.com/pages/the-network-project/","summary":"Papers TENET Model ","title":"the network project"},{"content":"Value at Risk For a given $$p$$,\nit is the worst case scenario, discounting the other worse cases whose total probability is p. Basically, the quantile of the associated distributions of the return. Critisism Does not tell you what is the extent of the loss in case of the bad thing happens.\nSolution: Conditional VaR\nAlso, called Expected Shortfall.\nThe conditional comes from conditioning on the VaR breach.\nThe expected value of the returns given a VaR breach, where X is the returns\n$$E [X | X \u0026lt; VaR]=\\int^{VaR} x f(x | x \u0026lt; VaR) dx$$ How to calculate bhaiii.\nComputation Assume normality, and then do qnorm xD.\nFancy porfolio theory, baby.\nVaR can be estimated either parametrically or nonparametrically.\nQuantile regression\nSemi-parametric method. Bayesian?\n","permalink":"https://example.com/pages/var/","summary":"Value at Risk For a given $$p$$,\nit is the worst case scenario, discounting the other worse cases whose total probability is p. Basically, the quantile of the associated distributions of the return. Critisism Does not tell you what is the extent of the loss in case of the bad thing happens.\nSolution: Conditional VaR\nAlso, called Expected Shortfall.\nThe conditional comes from conditioning on the VaR breach.\nThe expected value of the returns given a VaR breach, where X is the returns","title":"VaR"}]